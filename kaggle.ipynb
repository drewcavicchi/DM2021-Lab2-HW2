{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db18805-0e97-4282-9069-e97875fb1282",
   "metadata": {},
   "source": [
    "# Kaggle Competition Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5c4e4b-8ff6-4750-8eb0-f9029595895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "\n",
    "\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import emoji\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e77d110-f453-4ee2-b75b-e72e43160c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id identification\n",
       "0  0x28cc61           test\n",
       "1  0x29e452          train\n",
       "2  0x2b3819          train\n",
       "3  0x2db41f           test\n",
       "4  0x2a2acc          train"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pulling in data and taking a look\n",
    "\n",
    "data_identification = pd.read_csv(\"kaggle/data_identification.csv\")\n",
    "data_identification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44996b94-706e-41b4-adda-cab87f16cb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x3140b1</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x368b73</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x296183</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2bd6e1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2ee1dd</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       emotion\n",
       "0  0x3140b1       sadness\n",
       "1  0x368b73       disgust\n",
       "2  0x296183  anticipation\n",
       "3  0x2bd6e1           joy\n",
       "4  0x2ee1dd  anticipation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pulling in data and taking a look\n",
    "\n",
    "emotion = pd.read_csv(\"kaggle/emotion.csv\")\n",
    "emotion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba71062a-a919-49b1-88ee-a73244e54b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Below is some preprocessing, commented out because I exported the output to csv to avoid the long processing time\n",
    "\n",
    "\n",
    "# Taking the weird nested json format and flattening\n",
    "# tweets_source = tweets['_source'].tolist()\n",
    "# for i in range(len(tweets_source)):\n",
    "#     tweets_source[i] = tweets_source[i]['tweet']\n",
    "# tweets_source = pd.DataFrame(tweets_source)\n",
    "# def json_to_series(text):\n",
    "#     keys, values = zip(*[item for dct in json.loads(json.dumps(text)) for item in dct.items()])\n",
    "#     return pd.Series(values, index=keys)\n",
    "\n",
    "# Merging the flattened data and original data\n",
    "# tweets_expanded = pd.concat([tweets, tweets_source], axis=1)\n",
    "# tweets_merged = pd.merge(tweets_expanded, data_identification, on=\"tweet_id\", how='left')\n",
    "# tweets_merged = pd.merge(tweets_merged, emotion, on='tweet_id', how='left')\n",
    "\n",
    "# Using the emoji module to replace emoji with actual text for BERT\n",
    "# tweets_merged['text'] = tweets_merged['text'].apply(emoji.demojize, delimiters=(\"\", \"\"))\n",
    "\n",
    "# Turned into a csv to save time\n",
    "tweets_merged = pd.read_csv(\"tweets_processed.csv\")\n",
    "\n",
    "# Setting up train and test splits\n",
    "tweets_train_eval = tweets_merged[tweets_merged['identification'] == 'train']\n",
    "tweets_test = tweets_merged[tweets_merged['identification'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19512cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export processed tweets to csv to avoid having to reprocess\n",
    "# tweets_merged.to_csv(\"tweets_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f4f3a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drewc\\AppData\\Local\\Temp/ipykernel_35748/2417315730.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_train_eval['label'] = mle.transform(tweets_train_eval['emotion']).tolist()\n",
      "C:\\Users\\drewc\\AppData\\Local\\Temp/ipykernel_35748/2417315730.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_train_eval['data_type'] = ['not_set']*tweets_train_eval.shape[0]\n",
      "c:\\Users\\drewc\\Documents\\python\\fall_2021\\data_mining\\DM2021-Lab2-master\\dm_lab2\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_source</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">anger</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>33887</td>\n",
       "      <td>33887</td>\n",
       "      <td>33887</td>\n",
       "      <td>33887</td>\n",
       "      <td>33887</td>\n",
       "      <td>33887</td>\n",
       "      <td>33887</td>\n",
       "      <td>33887</td>\n",
       "      <td>33887</td>\n",
       "      <td>33887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>5980</td>\n",
       "      <td>5980</td>\n",
       "      <td>5980</td>\n",
       "      <td>5980</td>\n",
       "      <td>5980</td>\n",
       "      <td>5980</td>\n",
       "      <td>5980</td>\n",
       "      <td>5980</td>\n",
       "      <td>5980</td>\n",
       "      <td>5980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">anticipation</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>211595</td>\n",
       "      <td>211595</td>\n",
       "      <td>211595</td>\n",
       "      <td>211595</td>\n",
       "      <td>211595</td>\n",
       "      <td>211595</td>\n",
       "      <td>211595</td>\n",
       "      <td>211595</td>\n",
       "      <td>211595</td>\n",
       "      <td>211595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>37340</td>\n",
       "      <td>37340</td>\n",
       "      <td>37340</td>\n",
       "      <td>37340</td>\n",
       "      <td>37340</td>\n",
       "      <td>37340</td>\n",
       "      <td>37340</td>\n",
       "      <td>37340</td>\n",
       "      <td>37340</td>\n",
       "      <td>37340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">disgust</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>118236</td>\n",
       "      <td>118236</td>\n",
       "      <td>118236</td>\n",
       "      <td>118236</td>\n",
       "      <td>118236</td>\n",
       "      <td>118236</td>\n",
       "      <td>118236</td>\n",
       "      <td>118236</td>\n",
       "      <td>118236</td>\n",
       "      <td>118236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>20865</td>\n",
       "      <td>20865</td>\n",
       "      <td>20865</td>\n",
       "      <td>20865</td>\n",
       "      <td>20865</td>\n",
       "      <td>20865</td>\n",
       "      <td>20865</td>\n",
       "      <td>20865</td>\n",
       "      <td>20865</td>\n",
       "      <td>20865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">fear</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>train</th>\n",
       "      <td>54399</td>\n",
       "      <td>54399</td>\n",
       "      <td>54399</td>\n",
       "      <td>54399</td>\n",
       "      <td>54399</td>\n",
       "      <td>54399</td>\n",
       "      <td>54399</td>\n",
       "      <td>54399</td>\n",
       "      <td>54399</td>\n",
       "      <td>54399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>9600</td>\n",
       "      <td>9600</td>\n",
       "      <td>9600</td>\n",
       "      <td>9600</td>\n",
       "      <td>9600</td>\n",
       "      <td>9600</td>\n",
       "      <td>9600</td>\n",
       "      <td>9600</td>\n",
       "      <td>9600</td>\n",
       "      <td>9600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">joy</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>train</th>\n",
       "      <td>438614</td>\n",
       "      <td>438614</td>\n",
       "      <td>438614</td>\n",
       "      <td>438614</td>\n",
       "      <td>438614</td>\n",
       "      <td>438614</td>\n",
       "      <td>438614</td>\n",
       "      <td>438614</td>\n",
       "      <td>438614</td>\n",
       "      <td>438614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>77403</td>\n",
       "      <td>77403</td>\n",
       "      <td>77403</td>\n",
       "      <td>77403</td>\n",
       "      <td>77403</td>\n",
       "      <td>77403</td>\n",
       "      <td>77403</td>\n",
       "      <td>77403</td>\n",
       "      <td>77403</td>\n",
       "      <td>77403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">sadness</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>train</th>\n",
       "      <td>164421</td>\n",
       "      <td>164421</td>\n",
       "      <td>164421</td>\n",
       "      <td>164421</td>\n",
       "      <td>164421</td>\n",
       "      <td>164421</td>\n",
       "      <td>164421</td>\n",
       "      <td>164421</td>\n",
       "      <td>164421</td>\n",
       "      <td>164421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>29016</td>\n",
       "      <td>29016</td>\n",
       "      <td>29016</td>\n",
       "      <td>29016</td>\n",
       "      <td>29016</td>\n",
       "      <td>29016</td>\n",
       "      <td>29016</td>\n",
       "      <td>29016</td>\n",
       "      <td>29016</td>\n",
       "      <td>29016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">surprise</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">6</th>\n",
       "      <th>train</th>\n",
       "      <td>41420</td>\n",
       "      <td>41420</td>\n",
       "      <td>41420</td>\n",
       "      <td>41420</td>\n",
       "      <td>41420</td>\n",
       "      <td>41420</td>\n",
       "      <td>41420</td>\n",
       "      <td>41420</td>\n",
       "      <td>41420</td>\n",
       "      <td>41420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>7309</td>\n",
       "      <td>7309</td>\n",
       "      <td>7309</td>\n",
       "      <td>7309</td>\n",
       "      <td>7309</td>\n",
       "      <td>7309</td>\n",
       "      <td>7309</td>\n",
       "      <td>7309</td>\n",
       "      <td>7309</td>\n",
       "      <td>7309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">trust</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">7</th>\n",
       "      <th>train</th>\n",
       "      <td>174656</td>\n",
       "      <td>174656</td>\n",
       "      <td>174656</td>\n",
       "      <td>174656</td>\n",
       "      <td>174656</td>\n",
       "      <td>174656</td>\n",
       "      <td>174656</td>\n",
       "      <td>174656</td>\n",
       "      <td>174656</td>\n",
       "      <td>174656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>30822</td>\n",
       "      <td>30822</td>\n",
       "      <td>30822</td>\n",
       "      <td>30822</td>\n",
       "      <td>30822</td>\n",
       "      <td>30822</td>\n",
       "      <td>30822</td>\n",
       "      <td>30822</td>\n",
       "      <td>30822</td>\n",
       "      <td>30822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Unnamed: 0  _score  _index  _source  _crawldate  \\\n",
       "emotion      label data_type                                                    \n",
       "anger        0     train           33887   33887   33887    33887       33887   \n",
       "                   val              5980    5980    5980     5980        5980   \n",
       "anticipation 1     train          211595  211595  211595   211595      211595   \n",
       "                   val             37340   37340   37340    37340       37340   \n",
       "disgust      2     train          118236  118236  118236   118236      118236   \n",
       "                   val             20865   20865   20865    20865       20865   \n",
       "fear         3     train           54399   54399   54399    54399       54399   \n",
       "                   val              9600    9600    9600     9600        9600   \n",
       "joy          4     train          438614  438614  438614   438614      438614   \n",
       "                   val             77403   77403   77403    77403       77403   \n",
       "sadness      5     train          164421  164421  164421   164421      164421   \n",
       "                   val             29016   29016   29016    29016       29016   \n",
       "surprise     6     train           41420   41420   41420    41420       41420   \n",
       "                   val              7309    7309    7309     7309        7309   \n",
       "trust        7     train          174656  174656  174656   174656      174656   \n",
       "                   val             30822   30822   30822    30822       30822   \n",
       "\n",
       "                               _type  hashtags  tweet_id    text  \\\n",
       "emotion      label data_type                                       \n",
       "anger        0     train       33887     33887     33887   33887   \n",
       "                   val          5980      5980      5980    5980   \n",
       "anticipation 1     train      211595    211595    211595  211595   \n",
       "                   val         37340     37340     37340   37340   \n",
       "disgust      2     train      118236    118236    118236  118236   \n",
       "                   val         20865     20865     20865   20865   \n",
       "fear         3     train       54399     54399     54399   54399   \n",
       "                   val          9600      9600      9600    9600   \n",
       "joy          4     train      438614    438614    438614  438614   \n",
       "                   val         77403     77403     77403   77403   \n",
       "sadness      5     train      164421    164421    164421  164421   \n",
       "                   val         29016     29016     29016   29016   \n",
       "surprise     6     train       41420     41420     41420   41420   \n",
       "                   val          7309      7309      7309    7309   \n",
       "trust        7     train      174656    174656    174656  174656   \n",
       "                   val         30822     30822     30822   30822   \n",
       "\n",
       "                              identification  \n",
       "emotion      label data_type                  \n",
       "anger        0     train               33887  \n",
       "                   val                  5980  \n",
       "anticipation 1     train              211595  \n",
       "                   val                 37340  \n",
       "disgust      2     train              118236  \n",
       "                   val                 20865  \n",
       "fear         3     train               54399  \n",
       "                   val                  9600  \n",
       "joy          4     train              438614  \n",
       "                   val                 77403  \n",
       "sadness      5     train              164421  \n",
       "                   val                 29016  \n",
       "surprise     6     train               41420  \n",
       "                   val                  7309  \n",
       "trust        7     train              174656  \n",
       "                   val                 30822  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing, metrics, decomposition, pipeline, dummy\n",
    "\n",
    "# Using LabelEncoder to turn emotion labels into numeric representation\n",
    "\n",
    "mle = preprocessing.LabelEncoder()\n",
    "mle.fit(tweets_train_eval['emotion'])\n",
    "mle.classes_\n",
    "tweets_train_eval['label'] = mle.transform(tweets_train_eval['emotion']).tolist()\n",
    "\n",
    "# Tried out two ways to split test and eval, I preferred train_test_split because its a bit more clear\n",
    "# tweets_train, tweets_eval = np.split(tweets_train_eval.sample(frac=1, random_state = 99), [int(.8*len(tweets_train_eval))])\n",
    "tweets_train, tweets_eval, y_train, y_val = train_test_split(tweets_train_eval.index.values, \n",
    "        tweets_train_eval['label'].values, test_size=0.15, random_state=99, stratify = tweets_train_eval['label'].values)\n",
    "\n",
    "tweets_train_eval['data_type'] = ['not_set']*tweets_train_eval.shape[0]\n",
    "\n",
    "# Label train and val\n",
    "tweets_train_eval.loc[tweets_train, 'data_type'] = \"train\"\n",
    "tweets_train_eval.loc[tweets_eval, 'data_type'] = \"val\"\n",
    "\n",
    "# Taking a look at counts per train and val\n",
    "tweets_train_eval.groupby(['emotion', 'label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6daab66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy             516017\n",
       "anticipation    248935\n",
       "trust           205478\n",
       "sadness         193437\n",
       "disgust         139101\n",
       "fear             63999\n",
       "surprise         48729\n",
       "anger            39867\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at overall emotion counts. It is imbalanced, but totals are high so not super worried\n",
    "\n",
    "tweets_train_eval['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b9371ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pre-trained BERT through the Transformers library\n",
    "# I tried a few different pretrained models (regular bert, bert large, roberta large, and XLnet)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Setting up the encoding. I followed a few different tutorials on how to do this, but found batch_encode_plus was most convenient\n",
    "\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    tweets_train_eval[tweets_train_eval['data_type'] == \"train\"].text.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    padding='max_length', \n",
    "    max_length=90, \n",
    "    # Pytorch tensor\n",
    "    return_tensors='pt',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    tweets_train_eval[tweets_train_eval['data_type'] == \"val\"].text.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    padding=\"max_length\", \n",
    "    max_length=90, \n",
    "    # Pytorch tensor\n",
    "    return_tensors='pt',\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91a23660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "# include labels in the encoded data\n",
    "\n",
    "encoded_data_train['labels'] = tweets_train_eval[tweets_train_eval['data_type'] == \"train\"].label.values\n",
    "encoded_data_val['labels'] = tweets_train_eval[tweets_train_eval['data_type'] == \"val\"].label.values\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(tweets_train_eval[tweets_train_eval['data_type'] == \"train\"].label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(tweets_train_eval[tweets_train_eval['data_type'] == \"val\"].label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "# using DataLoader from pytorch to pre-load the data\n",
    "train_dataloader = DataLoader(dataset_train, shuffle=True, batch_size=256)\n",
    "val_dataloader = DataLoader(dataset_val, shuffle=True, batch_size=256)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a97f23d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler, AdamW\n",
    "\n",
    "# Setting up a dictionary to map label numeric values and actual values\n",
    "\n",
    "label_dict = dict(zip(mle.classes_, mle.transform(mle.classes_)))\n",
    "\n",
    "# Setting up the pretrained model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "# Using AdamW for the optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-05, \n",
    "                  eps=1e-08)\n",
    "                  \n",
    "# defining the learning rate scheduler\n",
    "num_epochs = 4\n",
    "num_training_steps = num_epochs* len(train_dataloader)\n",
    "scheduler = get_scheduler(\"linear\", optimizer = optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "135963bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 13/4833 [00:11<1:09:34,  1.15it/s, training_loss=0.367]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35748/3875624452.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mloss_train_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\drewc\\Documents\\python\\fall_2021\\data_mining\\DM2021-Lab2-master\\dm_lab2\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\drewc\\Documents\\python\\fall_2021\\data_mining\\DM2021-Lab2-master\\dm_lab2\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "seed_val = 1776\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "    f1 = f1_score_func(predictions, true_vals)\n",
    "    return f1\n",
    "\n",
    "\n",
    "print(\"starting training...\")\n",
    "    \n",
    "for epoch in (range(num_epochs)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_dataloader)\n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_description('Epoch {:1d}'.format(epoch))\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), 'finetuned_BERT_emoji_epoch_{}.model'.format(epoch))\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    f1 = evaluate(val_dataloader)\n",
    "    tqdm.write(\"epoch {} validation f1 score: {}\".format(epoch, f1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "363d1d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\drewc\\Documents\\python\\fall_2021\\data_mining\\DM2021-Lab2-master\\dm_lab2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2221: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Getting ready for prediction\n",
    "\n",
    "# Load model\n",
    "model.load_state_dict(torch.load('finetuned_BERT_emoji_epoch_0.model', map_location=torch.device('cpu')))\n",
    "\n",
    "# Encode test data\n",
    "encoded_data_test = tokenizer.batch_encode_plus(\n",
    "    tweets_test.text.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    padding='max_length', \n",
    "    max_length=90, \n",
    "    # Pytorch tensor\n",
    "    return_tensors='pt',\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1eb57098",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "\n",
    "\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test)\n",
    "\n",
    "test_dataloader = DataLoader(dataset_test, shuffle=True, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c3b3190",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "def testing(dataloader_test):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for batch in dataloader_test:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        logits = outputs[0]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "    \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "            \n",
    "    return predictions\n",
    "\n",
    "predictions = testing(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "156c1c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(predictions, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d3a4c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 2, 5, ..., 4, 7, 4], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4548c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411972, 11)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6e7ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "\n",
    "tweets_test.insert(0, \"predictions\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc352ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drewc\\AppData\\Local\\Temp/ipykernel_35748/448951895.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_test['predictions'] = tweets_test['predictions'].map(label_dict_inverse)\n"
     ]
    }
   ],
   "source": [
    "tweets_test['prediction'] = tweets_test['predictions'].map(label_dict_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "357e2d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867525</th>\n",
       "      <td>0x2913b4</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867529</th>\n",
       "      <td>0x2a980e</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id predictions\n",
       "2        0x28b412     sadness\n",
       "4        0x2de201     disgust\n",
       "9        0x218443     sadness\n",
       "30       0x2939d5        fear\n",
       "33       0x26289a         joy\n",
       "...           ...         ...\n",
       "1867525  0x2913b4    surprise\n",
       "1867529  0x2a980e       trust\n",
       "1867530  0x316b80         joy\n",
       "1867531  0x29d0cb       trust\n",
       "1867532  0x2a6a4f         joy\n",
       "\n",
       "[411972 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = tweets_test[['tweet_id', 'predictions']]\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ace73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.rename(columns={\"predictions\":\"emotion\", \"tweet_id\":\"id\"}, inplace=True)\n",
    "submission.to_csv('bert_emoji_epoch_6.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b78c9151d73f88aac7e9c3d17cdd5fd8c33ba4435a0d00d92b5957a07a3b6475"
  },
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
